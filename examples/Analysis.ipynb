{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file allows to analyze results obtained by running `experiments_nsc.py`.\n",
    "\n",
    "It computed performance metric, analyse the evolution of likelihood given number of clusters if available, and display the obtained clusters (for the selected methodology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../NSC/')\n",
    "sys.path.append('../DeepSurvivalMachines/')\n",
    "\n",
    "from ntc import datasets\n",
    "from NSC.experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to analyze other datasets result\n",
    "dataset = 'METABRIC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Results/' # Path where the data is saved\n",
    "x, t, e, covariates = datasets.load_dataset(dataset) # Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [0.25, 0.5, 0.75] # Horizons to evaluate the models\n",
    "times_eval = np.quantile(t[e > 0], horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.evaluation import EvalSurv\n",
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc, integrated_brier_score\n",
    "\n",
    "### Utils: The evaluatino metrics used\n",
    "def evaluate(survival, e = e, t = t,  times_eval = []):\n",
    "    folds = survival[('Use',)].values.flatten()\n",
    "    survival = survival.drop(columns = ['Use', 'Assignment'])\n",
    "    survival.columns = pd.MultiIndex.from_frame(pd.DataFrame(index=survival.columns).reset_index().astype(float))\n",
    "    times = survival.columns.get_level_values(1).unique()\n",
    "    results = {}\n",
    "\n",
    "    # If multiple risk, compute cause specific metrics\n",
    "    for r in survival.columns.get_level_values(0).unique():\n",
    "        for fold in np.arange(np.unique(folds).shape[0]):\n",
    "            res = {}\n",
    "            e_train, t_train = e[folds != fold], t[folds != fold]\n",
    "            e_test,  t_test  = e[folds == fold], t[folds == fold]\n",
    "\n",
    "            et_train = np.array([(e_train[i] == int(r), t_train[i]) for i in range(len(e_train))], # For estimation censoring\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "            et_test = np.array([(e_test[i] == int(r), t_test[i]) for i in range(len(e_test))], # For measure performance for given outcome\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "            \n",
    "            selection = (t_test < t_train.max()) | (e[folds == fold] != int(r))\n",
    "            \n",
    "            et_test = et_test[selection]\n",
    "            survival_train = survival[folds != fold][r]\n",
    "            survival_fold = survival[folds == fold][r]\n",
    "\n",
    "            km = EvalSurv(survival_train.T, t_train, e_train == int(r), censor_surv = 'km')\n",
    "            test_eval = EvalSurv(survival_fold.T, t_test, e_test == int(r), censor_surv = km)\n",
    "\n",
    "            res['Overall'] = {\n",
    "                    \"CIS\": test_eval.concordance_td(), \n",
    "                }\n",
    "            try:\n",
    "                res['Overall']['BRS'] = test_eval.integrated_brier_score(times.to_numpy())\n",
    "            except: pass\n",
    "\n",
    "            \n",
    "            if len(times_eval) > 0:\n",
    "                indexes = [np.argmin(np.abs(times - te)) for te in times_eval]\n",
    "                briers = brier_score(et_train, et_test, survival_fold[selection].iloc[:, indexes], times_eval)[1]\n",
    "                for te, brier, index in zip(times_eval, briers, indexes):\n",
    "                    try:\n",
    "                        res[te] = {\n",
    "                            \"CIS\": concordance_index_ipcw(et_train, et_test, 1 - survival_fold[selection].iloc[:, index], te)[0], \n",
    "                            \"BRS\": brier,\n",
    "                            \"ROCS\": cumulative_dynamic_auc(et_train, et_test, 1 - survival_fold[selection].iloc[:, index], te)[0][0]}\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "            results[(r, fold)] = pd.DataFrame.from_dict(res)\n",
    "    results = pd.concat(results)\n",
    "    results.index.set_names(['Risk', 'Fold', 'Metric'], inplace = True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and compute performance\n",
    "predictions, clusters, results, likelihood = {}, {}, {}, {}\n",
    "for file_name in os.listdir(path):\n",
    "    if dataset in file_name and '.csv' in file_name: \n",
    "        model = file_name       \n",
    "        model = model[model.index('_') + 1: model.index('.')]\n",
    "\n",
    "        print(\"Opening :\", file_name, ' - ', model)\n",
    "        predictions[model] = pd.read_csv(path + file_name, header = [0, 1], index_col = 0)\n",
    "        results[model] = evaluate(predictions[model], times_eval = times_eval)\n",
    "\n",
    "# Rename\n",
    "# TODO: Add your method in the list for nicer display\n",
    "dict_name = {'nsc': 'NSC', 'cox': 'CoxPH', 'ds': 'DeepSurv', 'dsm': 'DSM', 'dcm': 'DCM', 'dh': 'DeepHit', 'sumo': 'SuMo', 'st': 'Suvival Tree'} \n",
    "\n",
    "likelihood = pd.DataFrame.from_dict(likelihood, 'index').rename(dict_name)\n",
    "results = pd.concat(results).rename(dict_name)\n",
    "results.index.set_names('Model', 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = results.groupby(['Model', 'Risk', 'Metric']).apply(lambda x: pd.Series([\"{:.3f} ({:.2f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns))\n",
    "table = table.loc[table.index.get_level_values(2).isin(['CIS', 'BRS'])].unstack(level=-1).stack(level=0).unstack(level=-1).loc[:, ['CIS', 'BRS']]\n",
    "#table = table.loc[['NSC', 'DCM', 'SuMo', 'DSM', 'DeepHit', 'DeepSurv', 'CoxPH']]\n",
    "\n",
    "if len(table.index.get_level_values(1).unique()) == 1:\n",
    "    table = table.droplevel(1)\n",
    "else:\n",
    "    table = table.reorder_levels(['Risk', 'Model']).sort_index(level = 0, sort_remaining = False)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anlayze the outcome of the clustering method \n",
    "method_display = 'nsc' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models in family\n",
    "likelihood = {}\n",
    "for file_name in os.listdir(path):\n",
    "    if dataset in file_name and '.pickle' in file_name and method_display in file_name and 'k=' in file_name:\n",
    "        model = int(file_name[file_name.rindex('k=')+2: file_name.index('.')])\n",
    "        print(\"Likelihood Computation :\", file_name, ' - ', model)\n",
    "\n",
    "        model_pickle = Experiment.load(path + file_name)\n",
    "        likelihood[model] = model_pickle.likelihood(x, t, e)\n",
    "likelihood = pd.DataFrame.from_dict(likelihood, 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = likelihood.sort_index().mean(1)\n",
    "std = 1.96 * likelihood.sort_index().std(1) / np.sqrt(5)\n",
    "\n",
    "mean.plot()\n",
    "plt.fill_between(std.index, mean + std, mean - std, alpha = 0.3)\n",
    "plt.grid(alpha = .3)\n",
    "\n",
    "plt.xlabel(r'Number of clusters $K$')\n",
    "plt.ylabel(r'Negative Log Likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anlayze the outcome of the clustering method \n",
    "method_display = 'nsc_k=3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average cluster (can be for any method)\n",
    "assignment = {}\n",
    "ax = None\n",
    "for i in np.arange(5):\n",
    "    horizons_pred = np.linspace(0, 0.75, 10)\n",
    "    pred = predictions[method_display]\n",
    "    pred = pred[(pred.Use == i).values]\n",
    "\n",
    "    assignment[i] = pred.Assignment.idxmax(1)\n",
    "    pred = pred['1']\n",
    "    pred.columns = pred.columns.map(float)\n",
    "    ax = pred.groupby(assignment[i]).mean(0).T.plot(ax = ax, ls = '-')\n",
    "    \n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Predictions')\n",
    "plt.title('Average Cluster Across Folds')\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experiment associated - Only works when predcit_cluster is available\n",
    "for file_name in os.listdir(path):\n",
    "    if dataset in file_name and method_display + '.pickle' in file_name:\n",
    "        print(\"Cluster Computation :\", file_name)\n",
    "\n",
    "        model_pickle = Experiment.load(path + file_name)\n",
    "        clusters = model_pickle.survival_cluster(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "for i in clusters:\n",
    "    ax = pd.DataFrame(clusters[i], index = model_pickle.times).plot(ax = ax)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Predictions')\n",
    "plt.title('Estimated Cluster')\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average clusters across dataset\n",
    "average, ordering = {}, {}\n",
    "for fold in clusters:\n",
    "    horizons_pred = np.linspace(0, 0.75, 10)\n",
    "    average[fold] = pd.DataFrame(clusters[fold], index = model_pickle.times).rename_axis('Cluster')\n",
    "    ordering[fold] = {i: j for j, i in enumerate(average[fold].iloc[-1].sort_values().index)}\n",
    "    average[fold] = average[fold].rename(index = ordering[fold])\n",
    "else:\n",
    "    try: \n",
    "        average = pd.concat(average, names = ['Fold'])\n",
    "        mean = average.groupby('Cluster').mean()\n",
    "        confidence = 1.96 * average.groupby('Cluster').std() / len(average.index.get_level_values('Fold').unique())\n",
    "        ax = mean.plot()\n",
    "        for c, color in zip(mean.columns, list(mcolors.TABLEAU_COLORS.values())[:len(mean.columns)]):\n",
    "            ax.fill_between(mean.index, (mean[c] - confidence[c]), (mean[c] + confidence[c]), color = color, alpha = .1)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Survival Probability')\n",
    "        plt.grid(alpha = 0.3)\n",
    "        plt.legend(title = 'Clusters')\n",
    "        plt.show()\n",
    "    except:\n",
    "        print('Not same number of clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Kaplan Meier of patients in these clusters\n",
    "from lifelines import KaplanMeierFitter\n",
    "km_estimate = {}\n",
    "for fold in clusters:\n",
    "    selection = (predictions[method_display].Use == fold).values.flatten()\n",
    "    assignment = predictions[method_display][selection].Assignment.idxmax(1)\n",
    "    km_estimate[fold] = {}\n",
    "    for i in assignment.unique():\n",
    "        km = KaplanMeierFitter().fit(t[selection][assignment == i], e[selection][assignment == i])\n",
    "        km_estimate[fold][i] = km.survival_function_at_times(model_pickle.times)\n",
    "    km_estimate[fold] = pd.concat(km_estimate[fold], axis = 1).rename_axis('Cluster').rename(index = ordering[fold])\n",
    "\n",
    "ax = mean.plot(ls = '--')\n",
    "km_estimate = pd.concat(km_estimate, names = ['Fold'])\n",
    "mean = km_estimate.groupby('Cluster').mean()\n",
    "confidence = 1.96 * km_estimate.groupby('Cluster').std() / len(km_estimate.index.get_level_values('Fold').unique())\n",
    "mean.plot(ax = ax, color = {i: list(mcolors.TABLEAU_COLORS.values())[int(i)] for i in mean.columns})\n",
    "for c, color in zip(mean.columns, list(mcolors.TABLEAU_COLORS.values())[:len(mean.columns)]):\n",
    "    ax.fill_between(mean.index, (mean[c] - confidence[c]), (mean[c] + confidence[c]), color = color, alpha = .1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Estimate')\n",
    "plt.grid(alpha = 0.3)\n",
    "\n",
    "handle = [Line2D([0], [0],ls = '--'), Line2D([0], [0], ls = '-')]\n",
    "plt.legend(handle, ['Kaplan-Meier', 'Estimated Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distribution of probability to be part of a given cluster ?\n",
    "clusters_assignment = {}\n",
    "for fold in clusters:\n",
    "    selection = (predictions[method_display].Use == fold).values.flatten()\n",
    "    clusters_assignment[fold] = predictions[method_display][selection].Assignment.rename(index = ordering[fold])\n",
    "\n",
    "clusters_assignment = pd.concat(clusters_assignment, axis = 0)\n",
    "for cluster in clusters_assignment.columns:\n",
    "    clusters_assignment[cluster].plot.hist(alpha = 0.5, bins = 100)\n",
    "plt.xlabel('Probality cluster')\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.legend(title = 'Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Distribution maximally assigned\n",
    "axes = clusters_assignment.groupby(clusters_assignment.apply(lambda x: np.argmax(x), axis = 1)).boxplot(layout = (1, 3), figsize = (7, 3), grid = 0.5)\n",
    "for ax in axes:\n",
    "    ax.grid(alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average life expectancy for each cluster\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "clusters_expectancy = []\n",
    "for fold in clusters:\n",
    "    cluster_fold = clusters_assignment.loc[fold].copy()\n",
    "    cluster_fold['Assignment'] = cluster_fold.T.idxmax().T\n",
    "    cluster_fold['Time'] = t[cluster_fold.index]\n",
    "    cluster_fold['Event'] = e[cluster_fold.index]\n",
    "    clusters_expectancy.append(cluster_fold.groupby('Assignment').apply(lambda x: KaplanMeierFitter().fit(x['Time'], x['Event']).median_survival_time_))\n",
    "    print(multivariate_logrank_test(cluster_fold['Time'], cluster_fold['Assignment'], cluster_fold['Event']))\n",
    "    print(cluster_fold.groupby('Assignment').mean())\n",
    "    print(cluster_fold.groupby('Assignment').count())\n",
    "clusters_expectancy = pd.concat(clusters_expectancy, 1).replace([np.inf, -np.inf], np.nan)\n",
    "clusters_expectancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Assignment Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only available for NSC\n",
    "for file_name in os.listdir(path):\n",
    "    if dataset in file_name and method_display + '.pickle' in file_name:\n",
    "        print(\"Importance Computation :\", file_name)\n",
    "\n",
    "        model_pickle = Experiment.load(path + file_name)\n",
    "        importance = model_pickle.importance(x, t, e, n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in importance:\n",
    "    importance[fold] = pd.Series(importance[fold][0])\n",
    "importance = pd.concat(importance, 1, names = ['Fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display importance of features obtained by test\n",
    "importance.index = covariates\n",
    "importance.mean(1).sort_values().plot.bar(yerr = importance.std(1))\n",
    "plt.xlabel('Covariate')\n",
    "plt.ylabel('Likelihood change')\n",
    "plt.grid(alpha = 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
